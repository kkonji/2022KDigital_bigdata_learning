{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### uci 신용카드 자료 분석\n",
    "- PCA로 불필요한 컬럼 제거 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.applications import VGG16\n",
    "from keras.utils import plot_model # 모델 플롯 그리기\n",
    "from keras.utils import set_random_seed # 랜덤 시드 설정\n",
    "from keras.models import save_model, load_model # 모델 저장 및 로드\n",
    "from sklearn.decomposition import PCA, NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>220000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>88004</td>\n",
       "      <td>31237</td>\n",
       "      <td>15980</td>\n",
       "      <td>8500</td>\n",
       "      <td>20000</td>\n",
       "      <td>5003</td>\n",
       "      <td>3047</td>\n",
       "      <td>5000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>150000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8979</td>\n",
       "      <td>5190</td>\n",
       "      <td>0</td>\n",
       "      <td>1837</td>\n",
       "      <td>3526</td>\n",
       "      <td>8998</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>30000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20878</td>\n",
       "      <td>20582</td>\n",
       "      <td>19357</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22000</td>\n",
       "      <td>4200</td>\n",
       "      <td>2000</td>\n",
       "      <td>3100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>80000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>52774</td>\n",
       "      <td>11855</td>\n",
       "      <td>48944</td>\n",
       "      <td>85900</td>\n",
       "      <td>3409</td>\n",
       "      <td>1178</td>\n",
       "      <td>1926</td>\n",
       "      <td>52964</td>\n",
       "      <td>1804</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>36535</td>\n",
       "      <td>32428</td>\n",
       "      <td>15313</td>\n",
       "      <td>2078</td>\n",
       "      <td>1800</td>\n",
       "      <td>1430</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "ID                                                                            \n",
       "1          20000    2          2         1   24      2      2     -1     -1   \n",
       "2         120000    2          2         2   26     -1      2      0      0   \n",
       "3          90000    2          2         2   34      0      0      0      0   \n",
       "4          50000    2          2         1   37      0      0      0      0   \n",
       "5          50000    1          2         1   57     -1      0     -1      0   \n",
       "...          ...  ...        ...       ...  ...    ...    ...    ...    ...   \n",
       "29996     220000    1          3         1   39      0      0      0      0   \n",
       "29997     150000    1          3         2   43     -1     -1     -1     -1   \n",
       "29998      30000    1          2         2   37      4      3      2     -1   \n",
       "29999      80000    1          3         1   41      1     -1      0      0   \n",
       "30000      50000    1          2         1   46      0      0      0      0   \n",
       "\n",
       "       PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  \\\n",
       "ID            ...                                                        \n",
       "1         -2  ...          0          0          0         0       689   \n",
       "2          0  ...       3272       3455       3261         0      1000   \n",
       "3          0  ...      14331      14948      15549      1518      1500   \n",
       "4          0  ...      28314      28959      29547      2000      2019   \n",
       "5          0  ...      20940      19146      19131      2000     36681   \n",
       "...      ...  ...        ...        ...        ...       ...       ...   \n",
       "29996      0  ...      88004      31237      15980      8500     20000   \n",
       "29997      0  ...       8979       5190          0      1837      3526   \n",
       "29998      0  ...      20878      20582      19357         0         0   \n",
       "29999      0  ...      52774      11855      48944     85900      3409   \n",
       "30000      0  ...      36535      32428      15313      2078      1800   \n",
       "\n",
       "       PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  \n",
       "ID                                                                         \n",
       "1             0         0         0         0                           1  \n",
       "2          1000      1000         0      2000                           1  \n",
       "3          1000      1000      1000      5000                           0  \n",
       "4          1200      1100      1069      1000                           0  \n",
       "5         10000      9000       689       679                           0  \n",
       "...         ...       ...       ...       ...                         ...  \n",
       "29996      5003      3047      5000      1000                           0  \n",
       "29997      8998       129         0         0                           0  \n",
       "29998     22000      4200      2000      3100                           1  \n",
       "29999      1178      1926     52964      1804                           1  \n",
       "30000      1430      1000      1000      1000                           1  \n",
       "\n",
       "[30000 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"default of credit card clients.xls\", skiprows=1, index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30000 entries, 1 to 30000\n",
      "Data columns (total 24 columns):\n",
      " #   Column                      Non-Null Count  Dtype\n",
      "---  ------                      --------------  -----\n",
      " 0   LIMIT_BAL                   30000 non-null  int64\n",
      " 1   SEX                         30000 non-null  int64\n",
      " 2   EDUCATION                   30000 non-null  int64\n",
      " 3   MARRIAGE                    30000 non-null  int64\n",
      " 4   AGE                         30000 non-null  int64\n",
      " 5   PAY_0                       30000 non-null  int64\n",
      " 6   PAY_2                       30000 non-null  int64\n",
      " 7   PAY_3                       30000 non-null  int64\n",
      " 8   PAY_4                       30000 non-null  int64\n",
      " 9   PAY_5                       30000 non-null  int64\n",
      " 10  PAY_6                       30000 non-null  int64\n",
      " 11  BILL_AMT1                   30000 non-null  int64\n",
      " 12  BILL_AMT2                   30000 non-null  int64\n",
      " 13  BILL_AMT3                   30000 non-null  int64\n",
      " 14  BILL_AMT4                   30000 non-null  int64\n",
      " 15  BILL_AMT5                   30000 non-null  int64\n",
      " 16  BILL_AMT6                   30000 non-null  int64\n",
      " 17  PAY_AMT1                    30000 non-null  int64\n",
      " 18  PAY_AMT2                    30000 non-null  int64\n",
      " 19  PAY_AMT3                    30000 non-null  int64\n",
      " 20  PAY_AMT4                    30000 non-null  int64\n",
      " 21  PAY_AMT5                    30000 non-null  int64\n",
      " 22  PAY_AMT6                    30000 non-null  int64\n",
      " 23  default payment next month  30000 non-null  int64\n",
      "dtypes: int64(24)\n",
      "memory usage: 5.7 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.iloc[:,:23]\n",
    "target = df.iloc[:,23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 표준화 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = StandardScaler()\n",
    "st.fit(data)\n",
    "data = st.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "- n_components=0.95일때, 23개의 컬럼 -> 15개의 컬럼 감소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(data)\n",
    "data_pca = pca.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 8, 10, 11, 13, 15]\n"
     ]
    }
   ],
   "source": [
    "sl = []\n",
    "for i in np.arange(0.6, 1, 0.05):\n",
    "    pca = PCA(n_components=i)\n",
    "    pca.fit(data)\n",
    "    sl.append(pca.transform(data).shape[1])\n",
    "print(sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(sl, np.arange(0.6,1,0.05))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_target, test_target = train_test_split(data_pca, target, test_size=0.2, stratify=target, random_state=42)\n",
    "train_data2, val_data, train_target2, val_target = train_test_split(train_data, train_target,test_size=0.2, stratify=train_target, random_state=42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19200, 15)\n"
     ]
    }
   ],
   "source": [
    "print(train_data2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8117708333333333\n",
      "0.807\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_data2, train_target2)\n",
    "print(lr.score(train_data2,train_target2 ))\n",
    "print(lr.score(test_data,test_target ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "600/600 [==============================] - 2s 2ms/step - loss: 0.4880 - accuracy: 0.7920 - val_loss: 0.4739 - val_accuracy: 0.8046\n",
      "Epoch 2/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4543 - accuracy: 0.8097 - val_loss: 0.4681 - val_accuracy: 0.8104\n",
      "Epoch 3/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4496 - accuracy: 0.8117 - val_loss: 0.4631 - val_accuracy: 0.8092\n",
      "Epoch 4/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4470 - accuracy: 0.8129 - val_loss: 0.4600 - val_accuracy: 0.8098\n",
      "Epoch 5/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4445 - accuracy: 0.8147 - val_loss: 0.4604 - val_accuracy: 0.8094\n",
      "Epoch 6/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4431 - accuracy: 0.8148 - val_loss: 0.4573 - val_accuracy: 0.8100\n",
      "Epoch 7/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4419 - accuracy: 0.8155 - val_loss: 0.4552 - val_accuracy: 0.8094\n",
      "Epoch 8/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4410 - accuracy: 0.8154 - val_loss: 0.4545 - val_accuracy: 0.8106\n",
      "Epoch 9/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4397 - accuracy: 0.8165 - val_loss: 0.4542 - val_accuracy: 0.8112\n",
      "Epoch 10/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4391 - accuracy: 0.8164 - val_loss: 0.4535 - val_accuracy: 0.8112\n",
      "Epoch 11/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4386 - accuracy: 0.8164 - val_loss: 0.4553 - val_accuracy: 0.8138\n",
      "Epoch 12/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4379 - accuracy: 0.8166 - val_loss: 0.4504 - val_accuracy: 0.8119\n",
      "Epoch 13/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4370 - accuracy: 0.8174 - val_loss: 0.4504 - val_accuracy: 0.8100\n",
      "Epoch 14/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4367 - accuracy: 0.8166 - val_loss: 0.4493 - val_accuracy: 0.8121\n",
      "Epoch 15/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4363 - accuracy: 0.8165 - val_loss: 0.4499 - val_accuracy: 0.8108\n",
      "Epoch 16/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4359 - accuracy: 0.8179 - val_loss: 0.4491 - val_accuracy: 0.8100\n",
      "Epoch 17/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4354 - accuracy: 0.8180 - val_loss: 0.4488 - val_accuracy: 0.8108\n",
      "Epoch 18/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4348 - accuracy: 0.8177 - val_loss: 0.4484 - val_accuracy: 0.8119\n",
      "Epoch 19/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4347 - accuracy: 0.8178 - val_loss: 0.4509 - val_accuracy: 0.8112\n"
     ]
    }
   ],
   "source": [
    "md = Sequential()\n",
    "md.add(Dense(16, activation='selu', input_shape=(15,) ))\n",
    "md.add(Dense(8, activation='selu' ))\n",
    "md.add(Dense(4, activation='selu' ))\n",
    "md.add(Dense(2, activation='softmax'))\n",
    "md.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics='accuracy')\n",
    "es = EarlyStopping(monitor=\"val_accuracy\", min_delta=0, patience=8)\n",
    "history = md.fit(train_data2, train_target2, validation_data=(val_data, val_target), epochs=50, callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "def get_model(neuron_1, neuron_2,neuron_3,drop_rate, learning_rate, input_shape=(15,)):\n",
    "    # note that meta is a special argument that will be\n",
    "    # handed a dict containing input metadata\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neuron_1, activation='relu', input_shape=input_shape))\n",
    "    model.add(Dropout(drop_rate))\n",
    "    model.add(Dense(neuron_2, activation='relu'))\n",
    "    model.add(Dropout(drop_rate))\n",
    "    model.add(Dense(neuron_3, activation='relu'))\n",
    "    model.add(Dropout(drop_rate))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    return model\n",
    "clf = KerasClassifier(get_model,neuron_1=16, neuron_2=16,neuron_3=16, learning_rate=0.001, drop_rate=0.3, epochs=50, loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "600/600 [==============================] - 2s 2ms/step - loss: 0.5302\n",
      "Epoch 2/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4736\n",
      "Epoch 3/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4614\n",
      "Epoch 4/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4571\n",
      "Epoch 5/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4519\n",
      "Epoch 6/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4496\n",
      "Epoch 7/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4469\n",
      "Epoch 8/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4438\n",
      "Epoch 9/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4411\n",
      "Epoch 10/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4408\n",
      "Epoch 11/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4397\n",
      "Epoch 12/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4420\n",
      "Epoch 13/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4385\n",
      "Epoch 14/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4382\n",
      "Epoch 15/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4374\n",
      "Epoch 16/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4390\n",
      "Epoch 17/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4338\n",
      "Epoch 18/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4366\n",
      "Epoch 19/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4356\n",
      "Epoch 20/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4337\n",
      "Epoch 21/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4349\n",
      "Epoch 22/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4331\n",
      "Epoch 23/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4345\n",
      "Epoch 24/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4341\n",
      "Epoch 25/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4326\n",
      "Epoch 26/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4315\n",
      "Epoch 27/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4307\n",
      "Epoch 28/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4316\n",
      "Epoch 29/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4316\n",
      "Epoch 30/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4327\n",
      "Epoch 31/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4318\n",
      "Epoch 32/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4309\n",
      "Epoch 33/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4302\n",
      "Epoch 34/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4308\n",
      "Epoch 35/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4328\n",
      "Epoch 36/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4317\n",
      "Epoch 37/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4316\n",
      "Epoch 38/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4293\n",
      "Epoch 39/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4298\n",
      "Epoch 40/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4308\n",
      "Epoch 41/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4291\n",
      "Epoch 42/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4295\n",
      "Epoch 43/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4288\n",
      "Epoch 44/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4295\n",
      "Epoch 45/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4292\n",
      "Epoch 46/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4296\n",
      "Epoch 47/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4290\n",
      "Epoch 48/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4283\n",
      "Epoch 49/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4286\n",
      "Epoch 50/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4282\n",
      "0.8214062499999999 {'optimizer': 'adam', 'neuron_3': 48, 'neuron_2': 56, 'neuron_1': 40, 'loss': 'sparse_categorical_crossentropy', 'learning_rate': 0.001, 'drop_rate': 0.3}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"loss\": [\"sparse_categorical_crossentropy\"],\n",
    "    \"optimizer\": [\"adam\", \"sgd\"],\n",
    "    \"learning_rate\": [0.0001, 0.001, 0.01, 0.1],\n",
    "    \"neuron_1\": [16,24,32,40,48,56,64],\n",
    "    \"neuron_2\": [16,24,32,40,48,56,64],\n",
    "    \"neuron_3\": [16,24,32,40,48,56,64],\n",
    "    \"drop_rate\":[0.3,0.4,0.5,0.6,0.7]\n",
    "}\n",
    "rs = RandomizedSearchCV(clf, params, cv=3, scoring='accuracy', refit=True, return_train_score=True, n_jobs=-1, verbose=0)\n",
    "\n",
    "rs.fit(train_data2, train_target2)\n",
    "print(rs.best_score_, rs.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.5300\n",
      "Epoch 2/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4719\n",
      "Epoch 3/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4618\n",
      "Epoch 4/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4577\n",
      "Epoch 5/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4549\n",
      "Epoch 6/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4516\n",
      "Epoch 7/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4522\n",
      "Epoch 8/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4510\n",
      "Epoch 9/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4526\n",
      "Epoch 10/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4510\n",
      "Epoch 11/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4494\n",
      "Epoch 12/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4479\n",
      "Epoch 13/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4483\n",
      "Epoch 14/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4474\n",
      "Epoch 15/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4485\n",
      "Epoch 16/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4470\n",
      "Epoch 17/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4464\n",
      "Epoch 18/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4472\n",
      "Epoch 19/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4475\n",
      "Epoch 20/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4453\n",
      "Epoch 21/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4477\n",
      "Epoch 22/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4444\n",
      "Epoch 23/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4467\n",
      "Epoch 24/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4458\n",
      "Epoch 25/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4454\n",
      "Epoch 26/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4437\n",
      "Epoch 27/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4447\n",
      "Epoch 28/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4442\n",
      "Epoch 29/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4465\n",
      "Epoch 30/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4458\n",
      "Epoch 31/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4424\n",
      "Epoch 32/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4443\n",
      "Epoch 33/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4429\n",
      "Epoch 34/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4446\n",
      "Epoch 35/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4418\n",
      "Epoch 36/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4424\n",
      "Epoch 37/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4425\n",
      "Epoch 38/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4445\n",
      "Epoch 39/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4436\n",
      "Epoch 40/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4428\n",
      "Epoch 41/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4433\n",
      "Epoch 42/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4417\n",
      "Epoch 43/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4435\n",
      "Epoch 44/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4443\n",
      "Epoch 45/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4404\n",
      "Epoch 46/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4422\n",
      "Epoch 47/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4428\n",
      "Epoch 48/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4428\n",
      "Epoch 49/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4441\n",
      "Epoch 50/50\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.4436\n",
      "0.8203645833333333 {'optimizer': 'adam', 'neuron_n': 256, 'loss': 'sparse_categorical_crossentropy', 'learning_rate': 0.001, 'hidden_n': 0, 'drop_rate': 0.7}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "def get_model(hidden_n,neuron_n,drop_rate, learning_rate, input_shape=(15,)):\n",
    "    # note that meta is a special argument that will be\n",
    "    # handed a dict containing input metadata\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neuron_n, activation='relu', input_shape=input_shape))\n",
    "    model.add(Dropout(drop_rate))\n",
    "    for i in range(hidden_n):\n",
    "        model.add(Dense(neuron_n, activation='relu'))\n",
    "        model.add(Dropout(drop_rate))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    return model\n",
    "clf = KerasClassifier(get_model,neuron_n=16,hidden_n=3, learning_rate=0.001, drop_rate=0.3, epochs=50, loss='sparse_categorical_crossentropy')\n",
    "\n",
    "params = {\n",
    "    \"loss\": [\"sparse_categorical_crossentropy\"],\n",
    "    \"optimizer\": [\"adam\", \"sgd\"],\n",
    "    \"learning_rate\": [0.0001, 0.001, 0.01, 0.1],\n",
    "    \"hidden_n\": list(range(0,10)),\n",
    "    \"neuron_n\": [16,24,32,40,48,56,64,128,256,512],\n",
    "    \"drop_rate\":[0.3,0.4,0.5,0.6,0.7]\n",
    "}\n",
    "rs = RandomizedSearchCV(clf, params, cv=3, scoring='accuracy', refit=True, return_train_score=True, n_jobs=-1, verbose=0)\n",
    "\n",
    "rs.fit(train_data2, train_target2)\n",
    "print(rs.best_score_, rs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결론\n",
    "0.8214가 제일 높게 나온 기록이고, {'optimizer': 'adam', 'neuron_3': 48, 'neuron_2': 56, 'neuron_1': 40, 'loss': 'sparse_categorical_crossentropy', 'learning_rate': 0.001, 'drop_rate': 0.3}로 파라미터로 설정하였다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf92aa13fedf815d5c8dd192b8d835913fde3e8bc926b2a0ad6cc74ef2ba3ca2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
